{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def positional_encoding(\n",
    "    tensor, num_encoding_functions=6, include_input=True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies positional encoding to the input tensor.\n",
    "\n",
    "    Args:\n",
    "    tensor: Input tensor to be positionally encoded.\n",
    "    num_encoding_functions: Number of encoding functions used to compute the positional encoding.\n",
    "    include_input: Whether or not to include the input in the positional encoding.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Positionally encoded tensor.\n",
    "    \"\"\"\n",
    "    encoding = [tensor] if include_input else []\n",
    "\n",
    "    for i in range(num_encoding_functions):\n",
    "        for func in [torch.sin, torch.cos]:\n",
    "            encoding.append(func(2**i * tensor))\n",
    "\n",
    "    return torch.cat(encoding, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 130])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a test tensor, with batch_size 5 and each element is a timestep (between 0 and 10)\n",
    "x = torch.randint(0, 10, (1, 10))\n",
    "positional_encoding(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pe[:, : x.size(1), :].expand(x.size(0), -1, -1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 10, (1, 10))\n",
    "res = PositionalEncoding(20)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.embedder = nn.Linear(input_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc_time = PositionalEncoding(12)\n",
    "t = torch.randint(0, 10, (20, 5, 1))\n",
    "t = pos_enc_time(t)\n",
    "x = torch.randint(0, 10, (20, 5, 3 + 3 + 12))\n",
    "x = x.view(20, -1)\n",
    "x = x.to(dtype=torch.float)\n",
    "transformer_encoder = TransformerEncoder(x.shape[1], 256, 4, 2)\n",
    "transformer_encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
